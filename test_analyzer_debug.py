"""
test_analyzer_debug.py - Verifiering av analyzer_debug.py dashboard

Beskrivning:
    Testskript fÃ¶r att verifiera att analyzer_debug.py dashboard
    fungerar korrekt med alla komponenter och moduler.
    
Tester:
    - Dashboard kan instantieras
    - Alla moduler initialiseras korrekt
    - Simuleringsiterationer fungerar
    - Alla grafmetoder kan kÃ¶ras
    - Data flÃ¶dar genom systemet

AnvÃ¤ndning:
    python test_analyzer_debug.py
"""

import sys
import time
from analyzer_debug import AnalyzerDebugDashboard


def test_dashboard_creation():
    """Testar att dashboard kan skapas."""
    print("ğŸ§ª Test 1: Dashboard Creation")
    try:
        dashboard = AnalyzerDebugDashboard()
        print("  âœ… Dashboard created successfully")
        return dashboard
    except Exception as e:
        print(f"  âŒ Failed to create dashboard: {e}")
        sys.exit(1)


def test_module_initialization(dashboard):
    """Testar att alla moduler Ã¤r initialiserade."""
    print("\nğŸ§ª Test 2: Module Initialization")
    
    modules_to_check = [
        'message_bus', 'strategic_memory', 'meta_evolution', 'agent_manager',
        'feedback_router', 'feedback_analyzer', 'introspection_panel',
        'rl_controller', 'reward_tuner', 'strategy_engine', 'risk_manager',
        'decision_engine', 'execution_engine', 'portfolio_manager',
        'vote_engine', 'decision_simulator', 'consensus_engine',
        'timespan_tracker', 'action_chain_engine', 'system_monitor'
    ]
    
    for module_name in modules_to_check:
        if hasattr(dashboard, module_name):
            print(f"  âœ… {module_name} initialized")
        else:
            print(f"  âŒ {module_name} missing")
            sys.exit(1)
    
    print("  âœ… All modules initialized")


def test_simulation_iteration(dashboard):
    """Testar att simulering fungerar."""
    print("\nğŸ§ª Test 3: Simulation Iteration")
    try:
        dashboard.simulate_iteration()
        print("  âœ… Simulation iteration successful")
        print(f"  ğŸ“Š Iteration count: {dashboard.iteration_count}")
    except Exception as e:
        print(f"  âŒ Simulation failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def test_graph_creation(dashboard):
    """Testar att alla grafmetoder fungerar."""
    print("\nğŸ§ª Test 4: Graph Creation Methods")
    
    graph_methods = [
        'create_system_health_graph',
        'create_module_status_graph',
        'create_price_trends_graph',
        'create_indicator_graph',
        'create_decision_flow_graph',
        'create_reward_flow_graph',
        'create_parameter_evolution_graph',
        'create_agent_performance_graph',
        'create_agent_evolution_graph',
        'create_agent_metrics_graph',
        'create_event_log',
        'create_feedback_flow_graph',
        'create_timeline_graph',
        'create_portfolio_value_graph',
        'create_positions_graph',
        'create_portfolio_details'
    ]
    
    for method_name in graph_methods:
        try:
            method = getattr(dashboard, method_name)
            result = method()
            print(f"  âœ… {method_name}")
        except Exception as e:
            print(f"  âŒ {method_name} failed: {e}")
            sys.exit(1)
    
    print("  âœ… All graph methods working")


def test_data_flow(dashboard):
    """Testar dataflÃ¶de genom systemet."""
    print("\nğŸ§ª Test 5: Data Flow Through System")
    
    # Run a few iterations to generate data
    for i in range(5):
        dashboard.simulate_iteration()
    
    # Check that data is being collected
    checks = {
        'Portfolio status': lambda: dashboard.portfolio_manager.get_status(dashboard.current_prices),
        'Agent status': lambda: len(dashboard.introspection_panel.agent_status_history) > 0,
        'Feedback events': lambda: len(dashboard.introspection_panel.feedback_events) > 0,
        'System health': lambda: dashboard.system_monitor.get_system_health()['health_score'] > 0,
        'Strategic memory': lambda: dashboard.strategic_memory.generate_insights()['total_decisions'] > 0,
    }
    
    for check_name, check_func in checks.items():
        try:
            result = check_func()
            if result:
                print(f"  âœ… {check_name}: OK")
            else:
                print(f"  âš ï¸  {check_name}: No data yet (normal for new system)")
        except Exception as e:
            print(f"  âš ï¸  {check_name}: {e} (expected in some cases)")
    
    print("  âœ… Data flow verified")


def test_price_movement(dashboard):
    """Testar prisrÃ¶relser."""
    print("\nğŸ§ª Test 6: Price Movement")
    
    initial_prices = dashboard.current_prices.copy()
    
    # Run several iterations
    for i in range(10):
        dashboard.simulate_iteration()
    
    # Check that prices have changed
    price_changed = False
    for symbol in dashboard.symbols:
        if dashboard.current_prices[symbol] != initial_prices[symbol]:
            price_changed = True
            change = ((dashboard.current_prices[symbol] - initial_prices[symbol]) / 
                     initial_prices[symbol]) * 100
            print(f"  âœ… {symbol}: ${initial_prices[symbol]:.2f} â†’ "
                  f"${dashboard.current_prices[symbol]:.2f} ({change:+.2f}%)")
    
    if price_changed:
        print("  âœ… Price movement working")
    else:
        print("  âš ï¸  No price changes detected (unlikely but possible)")


def test_reward_flow(dashboard):
    """Testar reward flow frÃ¥n portfolio till RL."""
    print("\nğŸ§ª Test 7: Reward Flow (Sprint 4.4)")
    
    reward_metrics = dashboard.reward_tuner.get_reward_metrics()
    
    print(f"  ğŸ“Š Base rewards received: {len(reward_metrics['base_reward_history'])}")
    print(f"  ğŸ“Š Tuned rewards generated: {len(reward_metrics['tuned_reward_history'])}")
    
    if len(reward_metrics['base_reward_history']) > 0:
        print("  âœ… Reward flow active")
    else:
        print("  âš ï¸  No rewards yet (system just started)")


def test_adaptive_parameters(dashboard):
    """Testar adaptiva parametrar."""
    print("\nğŸ§ª Test 8: Adaptive Parameters (Sprint 4.3)")
    
    param_history = dashboard.introspection_panel.parameter_adjustments
    
    print(f"  ğŸ“Š Parameter adjustments: {len(param_history)}")
    
    # Check some key parameters
    params = {
        'signal_threshold': dashboard.strategy_engine.signal_threshold,
        'risk_tolerance': dashboard.risk_manager.risk_tolerance,
        'consensus_threshold': dashboard.decision_engine.consensus_threshold,
    }
    
    print("  ğŸ“Š Current parameter values:")
    for param_name, value in params.items():
        print(f"     {param_name}: {value:.4f}")
    
    print("  âœ… Adaptive parameters available")


def main():
    """Huvudfunktion fÃ¶r test."""
    print("="*70)
    print("ğŸ” Analyzer Debug Dashboard - Verification Tests")
    print("="*70)
    print()
    
    # Run all tests
    dashboard = test_dashboard_creation()
    test_module_initialization(dashboard)
    test_simulation_iteration(dashboard)
    test_graph_creation(dashboard)
    test_data_flow(dashboard)
    test_price_movement(dashboard)
    test_reward_flow(dashboard)
    test_adaptive_parameters(dashboard)
    
    print()
    print("="*70)
    print("âœ… All Tests Passed!")
    print("="*70)
    print()
    print("Dashboard is ready to use:")
    print("  python analyzer_debug.py")
    print("  â†’ Open http://localhost:8050")
    print()


if __name__ == "__main__":
    main()
