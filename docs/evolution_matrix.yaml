evolution_matrix:
  description: Matrix för agentutveckling och evolutionsmönster i Sprint 7
  
  evolution_triggers:
    performance_degradation:
      threshold: 0.15
      action: adjust_learning_rate
      resource_allocation: increase
      team_impact: notify_team
      
    consistent_improvement:
      threshold: 0.10
      action: promote_to_stable
      resource_allocation: maintain
      team_impact: share_knowledge
      
    resource_constraint:
      threshold: 0.80
      action: optimize_efficiency
      resource_allocation: redistribute
      team_impact: coordinate_usage
      
    team_synergy:
      threshold: 0.75
      action: form_specialist_team
      resource_allocation: group_optimize
      team_impact: enhance_collaboration
  
  agent_types:
    strategy_agent:
      evolution_path: [basic, intermediate, advanced, expert]
      resource_priority: high
      team_role: decision_maker
      synergies: [risk_agent, decision_agent]
      
    risk_agent:
      evolution_path: [conservative, balanced, adaptive, predictive]
      resource_priority: high
      team_role: risk_assessor
      synergies: [strategy_agent, portfolio_agent]
      
    decision_agent:
      evolution_path: [simple, weighted, consensus, adaptive]
      resource_priority: medium
      team_role: coordinator
      synergies: [strategy_agent, execution_agent]
      
    execution_agent:
      evolution_path: [basic, optimized, predictive, autonomous]
      resource_priority: medium
      team_role: executor
      synergies: [decision_agent, portfolio_agent]
      
    meta_parameter_agent:
      evolution_path: [static, adaptive, predictive, autonomous]
      resource_priority: low
      team_role: optimizer
      synergies: [all_agents]
      
    reward_tuner_agent:
      evolution_path: [basic, volatility_aware, overfitting_detect, advanced]
      resource_priority: medium
      team_role: reward_shaper
      synergies: [rl_controller, meta_parameter_agent]
  
  evolution_metrics:
    performance_gain:
      calculation: (current_performance - baseline_performance) / baseline_performance
      threshold_promotion: 0.10
      threshold_demotion: -0.15
      
    stability_score:
      calculation: 1.0 - (std_dev(recent_performance) / mean(recent_performance))
      threshold_stable: 0.85
      threshold_unstable: 0.60
      
    resource_efficiency:
      calculation: performance_gain / resource_consumed
      threshold_efficient: 1.2
      threshold_inefficient: 0.5
      
    team_contribution:
      calculation: weighted_average(individual_contribution, team_synergy)
      threshold_valuable: 0.75
      threshold_replace: 0.30
  
  resource_allocation_matrix:
    compute_resources:
      strategy_agent: 0.25
      risk_agent: 0.20
      decision_agent: 0.20
      execution_agent: 0.15
      meta_parameter_agent: 0.10
      reward_tuner_agent: 0.10
      
    memory_resources:
      strategic_memory: 0.30
      indicator_registry: 0.20
      introspection_panel: 0.20
      system_monitor: 0.15
      resource_planner: 0.10
      team_dynamics: 0.05
      
    training_priority:
      high: [strategy_agent, risk_agent]
      medium: [decision_agent, execution_agent, reward_tuner_agent]
      low: [meta_parameter_agent]
      adaptive: [all_agents_based_on_performance]
  
  team_composition_patterns:
    aggressive_trading:
      agents: [strategy_agent_expert, execution_agent_autonomous]
      resource_boost: 1.3
      risk_tolerance: high
      
    conservative_trading:
      agents: [risk_agent_predictive, decision_agent_consensus]
      resource_boost: 1.0
      risk_tolerance: low
      
    balanced_trading:
      agents: [strategy_agent_advanced, risk_agent_adaptive, decision_agent_weighted]
      resource_boost: 1.1
      risk_tolerance: medium
      
    exploration_phase:
      agents: [strategy_agent_intermediate, meta_parameter_agent_adaptive]
      resource_boost: 0.9
      risk_tolerance: medium
      focus: learning_and_adaptation
