# CI Pipeline Configuration
# Continuous Integration pipeline för RL/PPO + RewardTunerAgent system

ci_pipeline:
  name: NextGen AI Trader CI Pipeline
  version: 2.0
  description: |
    CI/CD pipeline för att validera RL/PPO-system, RewardTunerAgent,
    adaptiva parametrar och full systemintegration.

# Pipeline stages
stages:
  stage_1_code_quality:
    name: Code Quality Checks
    description: Linting, formatting och static analysis
    timeout: 5 minutes
    
    steps:
      - name: Python Linting
        tool: flake8
        config: .flake8
        targets:
          - modules/
          - tests/
        options:
          - --max-line-length=100
          - --ignore=E501,W503
        fail_on_error: true
      
      - name: Code Formatting Check
        tool: black
        config: pyproject.toml
        targets:
          - modules/
          - tests/
        options:
          - --check
          - --diff
        fail_on_error: false  # Warning only
      
      - name: Type Checking
        tool: mypy
        config: mypy.ini
        targets:
          - modules/reward_tuner.py
          - modules/rl_controller.py
        options:
          - --ignore-missing-imports
        fail_on_error: false  # Warning only
      
      - name: Security Scan
        tool: bandit
        targets:
          - modules/
        options:
          - -r
          - -ll
        fail_on_error: true
    
    success_criteria:
      - no_critical_lint_errors
      - no_security_vulnerabilities
  
  stage_2_unit_tests:
    name: Unit Tests
    description: Kör alla unit tests för enskilda moduler
    timeout: 10 minutes
    depends_on: [stage_1_code_quality]
    
    steps:
      - name: Install Dependencies
        command: pip install -r requirements.txt
        cache_dependencies: true
      
      - name: Run Reward Tuner Tests
        command: pytest tests/test_reward_tuner.py -v --tb=short
        expected_tests: 21
        min_pass_rate: 100%
        coverage_target: 90%
      
      - name: Run RL Controller Tests
        command: pytest tests/test_rl_controller.py -v --tb=short
        expected_tests: 11
        min_pass_rate: 100%
        coverage_target: 85%
      
      - name: Run Adaptive Parameters Tests
        command: pytest tests/test_adaptive_parameters_sprint4_3.py -v --tb=short
        expected_tests: 8
        min_pass_rate: 100%
        coverage_target: 85%
      
      - name: Run Strategic Memory Tests
        command: pytest tests/test_strategic_memory_engine.py -v --tb=short
        expected_tests: 14
        min_pass_rate: 100%
        coverage_target: 85%
      
      - name: Run Feedback Analyzer Tests
        command: pytest tests/test_feedback_analyzer.py -v --tb=short
        expected_tests: 23
        min_pass_rate: 100%
        coverage_target: 80%
      
      - name: Run Vote Engine Tests
        command: pytest tests/test_vote_engine.py -v --tb=short
        expected_tests: 12
        min_pass_rate: 90%  # Allow 1 failure
        coverage_target: 80%
      
      - name: Run Consensus Engine Tests
        command: pytest tests/test_consensus_engine.py -v --tb=short
        expected_tests: 14
        min_pass_rate: 100%
        coverage_target: 85%
      
      - name: Run Decision Simulator Tests
        command: pytest tests/test_decision_simulator.py -v --tb=short
        expected_tests: 12
        min_pass_rate: 100%
        coverage_target: 85%
    
    success_criteria:
      - min_90_percent_tests_pass
      - coverage_above_85_percent
      - no_test_crashes
    
    artifacts:
      - test_results.xml
      - coverage_report.html
  
  stage_3_integration_tests:
    name: Integration Tests
    description: Testa integration mellan moduler
    timeout: 15 minutes
    depends_on: [stage_2_unit_tests]
    
    steps:
      - name: Run Full System Integration Test
        command: pytest tests/test_sprint4_3_integration.py -v --tb=short
        expected_tests: 3
        min_pass_rate: 90%
        description: |
          Testar full integration med adaptiva parametrar genom hela systemet
      
      - name: Run Reward Flow Integration Test
        command: pytest tests/test_reward_tuner.py::test_RT005_full_reward_flow -v
        expected_tests: 1
        min_pass_rate: 100%
        description: |
          Verifierar portfolio → reward_tuner → rl_controller flow
      
      - name: Run Logging Integration Test
        command: pytest tests/test_reward_tuner.py::test_RT006_reward_logging_in_memory -v
        expected_tests: 1
        min_pass_rate: 100%
        description: |
          Verifierar logging i strategic_memory_engine
      
      - name: Run Visualization Integration Test
        command: pytest tests/test_reward_tuner.py::test_RT006_reward_visualization_in_panel -v
        expected_tests: 1
        min_pass_rate: 100%
        description: |
          Verifierar visualization i introspection_panel
      
      - name: Run Meta Evolution Integration Test
        command: pytest tests/test_meta_agent_evolution_engine.py -v --tb=short
        expected_tests: 9
        min_pass_rate: 100%
        description: |
          Verifierar meta agent evolution med adaptiva parametrar
      
      - name: Run Agent Manager Integration Test
        command: pytest tests/test_agent_manager.py -v --tb=short
        expected_tests: 7
        min_pass_rate: 100%
        description: |
          Verifierar agent versioning och profiler
    
    success_criteria:
      - all_integration_tests_pass
      - message_bus_communication_verified
      - full_reward_flow_verified
    
    artifacts:
      - integration_test_results.xml
      - system_logs.txt
  
  stage_4_system_validation:
    name: System Validation
    description: End-to-end system validation med live simulation
    timeout: 20 minutes
    depends_on: [stage_3_integration_tests]
    
    steps:
      - name: Run Demo Simulation (Sprint 4)
        command: python demo_sprint4.py --duration=60 --verify
        timeout: 90 seconds
        validation:
          - portfolio_manager_active
          - reward_tuner_processing
          - rl_controller_training
          - parameters_adjusting
        success_criteria:
          - base_rewards_generated > 10
          - tuned_rewards_generated > 10
          - reward_match_ratio == 1.0
          - parameter_adjustments_made > 5
      
      - name: Verify Reward Flow
        command: python verify_reward_flow.py
        timeout: 60 seconds
        validation:
          - base_reward_published
          - tuned_reward_published
          - rl_controller_received_reward
          - transformation_logged
        success_criteria:
          - reward_flow_complete
          - transformation_ratio_reasonable
      
      - name: Verify Parameter Adjustment Flow
        command: python -c "from modules.rl_controller import RLController; from modules.message_bus import MessageBus; bus = MessageBus(); controller = RLController(bus); controller.trigger_parameter_adjustment(); print('Parameter adjustment verified')"
        timeout: 30 seconds
        validation:
          - parameter_adjustment_published
          - modules_received_adjustment
        success_criteria:
          - all_modules_updated
      
      - name: Validate System Health
        command: python -m pytest tests/ -v --tb=short -k "not slow"
        timeout: 300 seconds
        validation:
          - test_pass_rate >= 97%
          - no_crashes
          - memory_usage_acceptable
        success_criteria:
          - overall_health_score > 0.9
    
    success_criteria:
      - demo_runs_successfully
      - reward_flow_verified
      - parameter_flow_verified
      - system_health_acceptable
    
    artifacts:
      - simulation_logs.txt
      - system_health_report.json
      - performance_metrics.json
  
  stage_5_performance_tests:
    name: Performance Tests
    description: Performance och load testing
    timeout: 15 minutes
    depends_on: [stage_4_system_validation]
    optional: true  # Kan skippa för snabbare CI
    
    steps:
      - name: Reward Transformation Performance
        command: python -m pytest tests/test_reward_tuner.py -v --benchmark
        metrics:
          - transformation_latency < 1ms
          - throughput > 1000 rewards/second
      
      - name: Parameter Adjustment Performance
        command: python -m pytest tests/test_rl_controller.py -v --benchmark
        metrics:
          - adjustment_latency < 5ms
          - throughput > 100 adjustments/second
      
      - name: Message Bus Performance
        metrics:
          - message_delivery_latency < 10ms
          - throughput > 10000 messages/second
      
      - name: Memory Usage
        metrics:
          - max_memory_usage < 500MB
          - no_memory_leaks
      
      - name: Load Test
        description: Simulera 1000 trades
        command: python load_test.py --trades=1000
        metrics:
          - completion_time < 60 seconds
          - error_rate < 0.01
    
    success_criteria:
      - all_performance_metrics_met
      - no_memory_leaks
      - acceptable_under_load
    
    artifacts:
      - performance_report.json
      - load_test_results.json
  
  stage_6_documentation:
    name: Documentation Validation
    description: Validera dokumentation och YAML-filer
    timeout: 5 minutes
    depends_on: [stage_1_code_quality]
    
    steps:
      - name: Validate YAML Files
        command: python -c "import yaml; import glob; [yaml.safe_load(open(f)) for f in glob.glob('docs/**/*.yaml', recursive=True)]"
        validation:
          - all_yaml_files_valid
          - no_syntax_errors
      
      - name: Check README Completeness
        validation:
          - sprint_status_documented
          - flowcharts_present
          - integration_documented
          - test_results_documented
      
      - name: Verify YAML Documentation
        files_required:
          - docs/reward_tuner_sprint4_4/adaptive_parameters.yaml
          - docs/reward_tuner_sprint4_4/feedback_loop.yaml
          - docs/reward_tuner_sprint4_4/functions.yaml
          - docs/reward_tuner_sprint4_4/reward_flowchart.yaml
          - docs/reward_tuner_sprint4_4/rl_reward_matrix.yaml
          - docs/reward_tuner_sprint4_4/rl_reward_summary.yaml
          - docs/reward_tuner_sprint4_4/rl_test_suite.yaml
          - docs/reward_tuner_sprint4_4/rl_trigger.yaml
          - docs/reward_tuner_sprint4_4/ci_pipeline.yaml
          - docs/reward_tuner_sprint4_4/ci_matrix.yaml
        validation:
          - all_required_files_exist
          - files_not_empty
          - yaml_syntax_valid
    
    success_criteria:
      - all_yaml_files_valid
      - documentation_complete
      - required_files_present

# Pipeline execution
execution:
  trigger_on:
    - push_to_main
    - push_to_feature_branch
    - pull_request
    - manual_trigger
  
  parallel_execution:
    stage_1_and_stage_6: can_run_parallel
    stage_2_unit_tests: parallel_within_stage
    other_stages: sequential
  
  failure_handling:
    on_stage_failure:
      - stop_pipeline
      - notify_team
      - create_github_issue
    
    retry_policy:
      max_retries: 2
      retry_on: [flaky_test, network_error]
      backoff: exponential
  
  notifications:
    on_success:
      - github_status: success
      - slack_channel: #ci-notifications
    
    on_failure:
      - github_status: failure
      - slack_channel: #ci-alerts
      - email: team@nextgen.ai
    
    on_warning:
      - github_status: warning
      - slack_channel: #ci-notifications

# Environment configuration
environments:
  ci_environment:
    python_version: "3.12"
    os: ubuntu-latest
    dependencies: requirements.txt
    environment_variables:
      - PYTHONPATH: /home/runner/work/NextGen/NextGen
      - CI: true
      - TEST_MODE: true
    
    resources:
      cpu: 2_cores
      memory: 4GB
      disk: 10GB
      timeout: 60_minutes

# Success criteria för hela pipeline
pipeline_success_criteria:
  must_pass:
    - stage_1_code_quality
    - stage_2_unit_tests
    - stage_3_integration_tests
    - stage_4_system_validation
    - stage_6_documentation
  
  optional:
    - stage_5_performance_tests
  
  overall_requirements:
    - test_pass_rate >= 97%
    - no_critical_errors
    - documentation_complete
    - yaml_files_valid
    - reward_flow_verified
    - parameter_flow_verified
  
  metrics_tracked:
    - total_tests_run
    - tests_passed
    - tests_failed
    - code_coverage
    - pipeline_duration
    - stage_durations
    - failure_rate_over_time

# Artifacts och rapporter
artifacts:
  test_results:
    - format: JUnit XML
    - retention: 30 days
    - upload_to: GitHub Actions
  
  coverage_reports:
    - format: HTML + JSON
    - retention: 30 days
    - upload_to: Codecov
  
  logs:
    - format: Plain text
    - retention: 7 days
    - upload_to: GitHub Actions
  
  performance_metrics:
    - format: JSON
    - retention: 90 days
    - upload_to: Internal metrics DB

# Integration med GitHub
github_integration:
  status_checks:
    required:
      - Code Quality
      - Unit Tests
      - Integration Tests
      - System Validation
      - Documentation
    
    optional:
      - Performance Tests
  
  branch_protection:
    require_status_checks: true
    require_code_review: 1_approver
    dismiss_stale_reviews: true
  
  auto_merge:
    enabled: false
    conditions:
      - all_checks_pass
      - approved_by_maintainer
      - no_conflicts
